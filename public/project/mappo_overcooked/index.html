<!DOCTYPE html>
<!-- This site was created with Hugo Blox. https://hugoblox.com -->
<!-- Last Published: April 22, 2024 --><html lang="en-us" >


<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Hugo Blox Builder 5.9.7" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css" integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css" integrity="" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  



























  
  
  






  <meta name="author" content="Jiawei Zhan" />





  

<meta name="description" content="Using multi-agent Proximal Policy Optimization to solve famous Overcooked game." />



<link rel="alternate" hreflang="en-us" href="http://localhost:1313/project/mappo_overcooked/" />
<link rel="canonical" href="http://localhost:1313/project/mappo_overcooked/" />



  <link rel="manifest" href="/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_180x180_fill_lanczos_center_3.png" />

<meta name="theme-color" content="#1565c0" />










  






<meta property="twitter:card" content="summary_large_image" />

  <meta property="twitter:site" content="@GetResearchDev" />
  <meta property="twitter:creator" content="@GetResearchDev" />
<meta property="twitter:image" content="http://localhost:1313/project/mappo_overcooked/featured.gif" />



  

<meta property="og:type" content="article" />
<meta property="og:site_name" content="Jiawei Zhan" />
<meta property="og:url" content="http://localhost:1313/project/mappo_overcooked/" />
<meta property="og:title" content="Multi-Agent Reinforcement Learning for Cooperative Games | Jiawei Zhan" />
<meta property="og:description" content="Using multi-agent Proximal Policy Optimization to solve famous Overcooked game." /><meta property="og:image" content="http://localhost:1313/project/mappo_overcooked/featured.gif" /><meta property="og:locale" content="en-us" />

  
    <meta
      property="article:published_time"
      content="2024-04-03T00:00:00&#43;00:00"
    />
  
  
    <meta property="article:modified_time" content="2024-04-03T00:00:00&#43;00:00">
  






    









<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/project/mappo_overcooked/"
  },
  "headline": "Multi-Agent Reinforcement Learning for Cooperative Games",
  
  "image": [
    "http://localhost:1313/project/mappo_overcooked/featured.gif"
  ],
  
  "datePublished": "2024-04-03T00:00:00Z",
  "dateModified": "2024-04-03T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Jiawei Zhan"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Jiawei Zhan",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "Using multi-agent Proximal Policy Optimization to solve famous Overcooked game."
}
</script>

  

  




  
  
  

  
  

  


  
  <title>Multi-Agent Reinforcement Learning for Cooperative Games | Jiawei Zhan</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="79aaf6febc6efb4875fe0b8b22342024" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Jiawei Zhan</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Jiawei Zhan</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#projects"><span>Projects</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#talks"><span>Talks</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#featured"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#contact"><span>Contact</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/uploads/Jiawei_Intern_resume_hpc_12.pdf"><span>CV</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    <article class="article article-project">

  






















  
  



<div class="article-container pt-3">
  <h1>Multi-Agent Reinforcement Learning for Cooperative Games</h1>

  

  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Apr 3, 2024
  </span>
  

  

  

  
  
  
  

  
  

</div>

  




<div class="btn-links mb-3">
  
  








  



<a class="btn btn-outline-primary btn-page-header" href="/project/mappo_overcooked/mappo_overcooked.pdf" target="_blank" rel="noopener">
  PDF
</a>




  
  
    
  
<a class="btn btn-outline-primary btn-page-header" href="https://github.com/JiaweiZhan/MAPPO_OvercookedGame" target="_blank" rel="noopener">
  Code
</a>














</div>


</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 675px; max-height: 475px;">
  <div style="position: relative">
    <img src="/project/mappo_overcooked/featured_huc1970c6f493abe724917acfca5fd7313_3064653_720x2500_fit_lanczos_1.gif" width="675" height="475" alt="" class="featured-image">
    <span class="article-header-caption">Two agents are cooking onion soups.</span>
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <h1 id="project-overview">Project Overview</h1>
<p>In the multi-agent <a href="https://github.com/HumanCompatibleAI/overcooked_ai" target="_blank" rel="noopener">Overcooked environment</a>, two chefs must collaborate in a restaurant to cook onion soups. The objective is to develop a reinforcement learning method that maximizes the number of soups delivered within an episode across various layouts. I found this problem particularly interesting and challenging, as traditional reinforcement learning approaches like Q-Learning and policy gradient are not well-suited for multi-agent environments. Consequently, I chose this project as a weekend endeavor to broaden my understanding of reinforcement learning. To address this challenge, I proposed using Multi-Agent Proximal Policy Optimization (MAPPO), which involves centralized training and decentralized execution techniques. Additionally, I implemented reward shaping and multiprocessing to enhance exploration efficiency. This approach proved highly effective in the Overcooked game, as demonstrated by the implementation details available in my <a href="https://github.com/JiaweiZhan/MAPPO_OvercookedGame" target="_blank" rel="noopener">Github repository</a>.</p>
<h1 id="theory">Theory</h1>
<h2 id="core-proximal-policy-optimization">Core: Proximal Policy Optimization</h2>
<p><a href="https://arxiv.org/abs/1707.06347" target="_blank" rel="noopener">Proximal Policy Optimization (PPO)</a> is trying to solve a question: how can we take the biggest possible improvement step on a policy without stepping so far from the old, using the data that are currently available. This&rsquo;s achieved by clipping in the objective function to prevent the new policy to get far from the old policy. Specifically, PPO update polices via</p>



\begin{equation}
  \theta_{k + 1}=\mathrm{argmax}_{\theta}E_{s, a\sim\pi_{\theta_k}}[L(s, a, \theta_k, \theta)],
\end{equation}

<p>by taking multiple steps of gradient decent. Here $L$ is given by:</p>



\begin{equation}
\begin{split}
L(s, a, \theta_k, \theta) = \mathrm{min}\begin{cases}\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_{k}}}(s, a), \\
\mathrm{clip}\left(\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}, 1-\epsilon, 1+\epsilon\right)A^{\pi_{\theta_{k}}}(s, a).\end{cases}
\end{split}
\end{equation}

<p>where hyperparameter $\epsilon$ controls how far away the new policy is allowed to go from the old. The advantage function $A^{\pi_{\theta}}(s, a)$ is approximated via <a href="https://arxiv.org/abs/1506.02438?source=post_page---------------------------" target="_blank" rel="noopener">Generalized Advantage Estimation</a>:
\begin{equation}
A_t^{\mathrm{GAE}(\gamma, \lambda)} = \sum_{l=0}^{\infty}(\gamma\lambda)^{l}\left(r_{t+l} + \gamma V^{\pi, \gamma}(s_{t + l + 1}) - V^{\pi, \gamma}(s_{t + l})\right).
\end{equation}
Within the advantage function is the value function $V^{\pi, \gamma}(s_t)$ approximated by another model, which is fitted by regression on mean-squared error:</p>



\begin{equation}
  \psi = \mathrm{argmin}_{\phi}||V_{\psi}^{\pi, \gamma}(s_t) - \sum_{l=0}^{\infty}\gamma^{l}r_{t + l}||^2,
\end{equation}

<p>through gradient descent algorithm.</p>
<h2 id="centralizing-training-and-decentralizing-execution">Centralizing Training and Decentralizing Execution</h2>
<p>The overcooked game is a cooperative, multi-agent games that may be challenging for traditional reinforcement learning approaches such as Q-Learning or policy gradien. Here, I decided to adopt <a href="https://arxiv.org/abs/1706.02275" target="_blank" rel="noopener">centralizing training and decentralizing execution technics</a> to achieve cooperation amoung multiple-agent in the game. More concretely, for a game with N agents with policies $\Pi={\pi_1, &hellip;, \pi_N}$ parameterized by $\Theta={\theta_1, &hellip;, \theta_N}$. The gradient of expected return $J(\theta_i)=E[R_{i}]$ for agent $i$ is:</p>



\begin{equation}
  \nabla_{\theta_{i}}J(\theta_i)=E_{o_i, a_i\sim\pi_i}[\nabla_{\theta_i}\mathrm{log}\pi_i(a_i|o_i)A_{i}^{\Pi}:(\mathbf{x}, a1, a2, ..., a_N)],
\end{equation}

<p>where $o_i$ is the local observation of agent $i$, and $\mathbf{x}$ is the global state of the game,  including all agents&rsquo; features $\mathbf{\{[player\ i\ features, player\ i\ position]\}}$. In other word, during the training procedure, the advantage function is approximated given the full information of the game and agents&rsquo; strategies are evolved based on that. Hopefully, agents can learn to coorperate and make decisions based on their local observation.</p>
<h1 id="implementation-details">Implementation Details</h1>
<p>The backbone PPO follows the implementation in the original <a href="https://arxiv.org/abs/1707.06347" target="_blank" rel="noopener">PPO paper</a>, utilizing a 4-layer policy model with 96, 64, 64, and 6 neurons, and a separate 4-layer value function with 48, 64, 64, and 1 neurons. This section discusses additional implementation aspects, including centralized training and decentralized execution, and outlines strategies to tackle challenges associated with sparse rewards and time-intensive online data collection.</p>
<h2 id="shared-policy-model-and-symmetric-value-function">Shared Policy Model and Symmetric Value Function</h2>
<p>The idea of shared policy is motivated by the pursuit of AGI in the whold AI field as well as the intuition of &lsquo;I should be able to know what to do if I&rsquo;m in his position.&rsquo; In that case, both agent in the Overcooked game will share a single policy game. And two agents are trying to optimized the unified strategy together:



\begin{equation}
  \theta_{k+1} = \mathrm{argmax}_{\theta}E_{i}\left[E_{s_i, a_i\sim\pi_{\theta_k}}[L(o_i, a_i, \theta_k, \theta)]\right]
\end{equation},

where $i$ is the agents&rsquo; index.</p>
<p>Additionally, the centralized value function should be invarient to the switch of two agents, since the agents should be indistinguishable and the global state should only depends on where the agents are located in as well what they are doing, instead who is doing that. Motivated by this the invarient value function can be expressed by:



\begin{equation}
  V_{\psi}(\mathbf{x}) = \mathbf{O}\{f_{\psi}(o_i)\},
\end{equation}

where $\mathbf{O}$ can be any invarient operator, such as mean and plus. In my implementation, I choose to use plus operator.</p>
<h2 id="reward-shaping">Reward Shaping</h2>
<p>In the default setting, agents receive rewards only if they successfully deliver onion soup, which requires a sequence of actions including placing onions in a pot and picking up the soup. To promote efficient learning of these prerequisites for soup-making, rewarding intermediate achievements could be beneficial. This strategy helps overcome the challenge of sparse rewards throughout the game, potentially leading to highly inefficient exploration processes. The reward formula I employ is: $ r = 20 \times$ soup delivery$ + 3 \times$ PLACEMENT IN POT REWARD $+ 3 \times $DISH PICKUP REWARD$ + 5 \times $SOUP PICKUP REWARD. The coefficients in this formula represent the cumulative score all agents receive when one successfully completes an action. Due to the centralized training techniques I use, the rewards are shared among all agents.</p>
<h2 id="efficient-exploration-with-parallel-multiple-walkers">Efficient Exploration with Parallel Multiple Walkers</h2>
<p>The exploration process significantly impacts the efficiency of learning in RL model training and can be time-consuming. To enhance efficiency, I have set up multiple independent environments allowing parallel exploration by creating several processes, each hosting an independent environment. These processes share the same policy model and value function. Gradients of weights are computed in parallel, then averaged and synchronized across processes. This setup enables all agents to explore the environments and collect data using the same strategy and value function.</p>
<h1 id="result">Result</h1>
<p>By combining all technics mentioned above, I&rsquo;m able to train MAPPO to deliver more than 7 onion soups for all 5 layouts in 400 step horizon. Fig.<a href="#pookie">train_eval</a> shows the number of soups delivered per episode during the training procedure(a), as well as the reliability of trained model by plotting the mean and variance of occurance of delivering soup for 100 consecutive episodes(b). More than that, thanks to the parallel multi-walker exploration strategy enabled by multi-processing with 32 CPUs, the RL model can be trained within 5 minutes per layout.</p>
<p align="center" name="pookie">
  <img alt="Light" src="./nsoup_train.png" width="65%">
  <img alt="Dark" src="./eval.png" width="65%">
</p>
<h1 id="demo">Demo</h1>
<ul>
<li><em><strong>asymmetric advantages layout</strong></em></li>
</ul>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="./asymmetric_advantages.gif" alt="asymmetric_advantages" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<ul>
<li><em><strong>forced coordination layout</strong></em></li>
</ul>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="./forced_coordination.gif" alt="forced_coordination" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<h1 id="discussion">Discussion</h1>
<p>I showed that MAPPO with centralizing training and decentralizing execution strategy achieves surprising effectiveness in cooperative, multi-agent Overcooked game. I also demonstrated the importance of reward structures in improving learning efficiency. Here are some additional explorations that I could possibly do in the future:</p>
<ul>
<li>Utilizing RNN for the policy and value functions.</li>
<li>Robustness and adversarial testing.</li>
</ul>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/deep-learning/">Deep Learning</a>
  
  <a class="badge badge-light" href="/tag/reinforcement-learning/">Reinforcement Learning</a>
  
  <a class="badge badge-light" href="/tag/high-performance-computing/">High-Performance Computing</a>
  
</div>



<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=http%3A%2F%2Flocalhost%3A1313%2Fproject%2Fmappo_overcooked%2F&amp;text=Multi-Agent&#43;Reinforcement&#43;Learning&#43;for&#43;Cooperative&#43;Games" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=http%3A%2F%2Flocalhost%3A1313%2Fproject%2Fmappo_overcooked%2F&amp;t=Multi-Agent&#43;Reinforcement&#43;Learning&#43;for&#43;Cooperative&#43;Games" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
        
      
      <li>
        <a href="mailto:?subject=Multi-Agent%20Reinforcement%20Learning%20for%20Cooperative%20Games&amp;body=http%3A%2F%2Flocalhost%3A1313%2Fproject%2Fmappo_overcooked%2F" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=http%3A%2F%2Flocalhost%3A1313%2Fproject%2Fmappo_overcooked%2F&amp;title=Multi-Agent&#43;Reinforcement&#43;Learning&#43;for&#43;Cooperative&#43;Games" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="whatsapp://send?text=Multi-Agent&#43;Reinforcement&#43;Learning&#43;for&#43;Cooperative&#43;Games%20http%3A%2F%2Flocalhost%3A1313%2Fproject%2Fmappo_overcooked%2F" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=http%3A%2F%2Flocalhost%3A1313%2Fproject%2Fmappo_overcooked%2F&amp;title=Multi-Agent&#43;Reinforcement&#43;Learning&#43;for&#43;Cooperative&#43;Games" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="http://localhost:1313/"><img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hu494af1733929cec4ea29856580d5cd9c_116288_270x270_fill_q75_lanczos_center.jpg" alt="Jiawei Zhan"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="http://localhost:1313/">Jiawei Zhan</a></h5>
      <h6 class="card-subtitle">Ph.D. candidate in quantum science and engineering</h6>
      <p class="card-text">My research interests include deep learning, quantum chemistry and high-performance computing.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:jiaweiz@uchicago.edu" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com.co/citations?user=dv0vs6cAAAAJ&amp;hl=vi" target="_blank" rel="noopener">
        <i class="fas fa-graduation-cap"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/JiaweiZhan" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/jiawei-zhan-11a99b1a3/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="/uploads/Jiawei_Intern_resume_hpc_12.pdf" >
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


















    <div class="project-related-pages content-widget-hr">
      
      

      
      
      

      
      
      

      
      
      
    </div>
  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  


<p class="powered-by">
  
  <a href="/privacy/">Privacy Policy</a>
  
  
   &middot; 
  <a href="/terms/">Terms</a>
  
</p>












  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2024 Me. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Hugo Blox Builder</a> — the free, <a href="https://github.com/HugoBlox/hugo-blox-builder" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.js"></script>




  

  
  

  






  <script src="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js" integrity="" crossorigin="anonymous"></script>








  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  












  
  
  
  
  
  
  

















<script id="page-data" type="application/json">{"use_headroom":true}</script>


  <script src="/js/wowchemy-headroom.js" type="module"></script>









  
  


<script src="/en/js/wowchemy.min.js"></script>



  <script src="/js/wowchemy-map.js" type="module"></script>




  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.js" type="module"></script>


















</body>
</html>
